{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Some functions for deep neural network\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)\n",
    "    \n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None \n",
    "    return max_norm\n",
    "\n",
    "#Fetching Mnist Data\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eac6d0c9f8096cd8eefcbe2affdf812e8a079505"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3141299ae15be93efd297f9166c6ee05742510a"
   },
   "source": [
    "## Dropout NN 3 Layers(ReLU) 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation set accuracy: 91.660% \tLoss: 0.30961\n",
      "Epoch: 5 \tValidation set accuracy: 95.980% \tLoss: 0.14728\n",
      "Epoch: 10 \tValidation set accuracy: 97.360% \tLoss: 0.09577\n",
      "Epoch: 15 \tValidation set accuracy: 97.660% \tLoss: 0.08058\n",
      "Epoch: 20 \tValidation set accuracy: 97.940% \tLoss: 0.07182\n",
      "Epoch: 25 \tValidation set accuracy: 98.020% \tLoss: 0.06575\n",
      "Epoch: 30 \tValidation set accuracy: 98.000% \tLoss: 0.06732\n",
      "Epoch: 35 \tValidation set accuracy: 98.040% \tLoss: 0.06660\n",
      "Epoch: 40 \tValidation set accuracy: 98.120% \tLoss: 0.06518\n",
      "Epoch: 45 \tValidation set accuracy: 98.180% \tLoss: 0.06644\n",
      "Epoch: 50 \tValidation set accuracy: 98.140% \tLoss: 0.06653\n",
      "Epoch: 55 \tValidation set accuracy: 98.060% \tLoss: 0.06866\n",
      "Epoch: 60 \tValidation set accuracy: 98.080% \tLoss: 0.07063\n",
      "Epoch: 65 \tValidation set accuracy: 98.020% \tLoss: 0.07107\n",
      "Epoch: 70 \tValidation set accuracy: 98.100% \tLoss: 0.07233\n",
      "Epoch: 75 \tValidation set accuracy: 98.120% \tLoss: 0.07203\n",
      "Epoch: 80 \tValidation set accuracy: 98.100% \tLoss: 0.07323\n",
      "Epoch: 85 \tValidation set accuracy: 98.100% \tLoss: 0.07401\n",
      "termination\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 1024\n",
    "n_hidden2 = 1024\n",
    "n_hidden3 = 1024\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, 0.8, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden2 = tf.layers.dense(tf.layers.dropout(hidden1, dropout_rate, training=training), n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu)\n",
    "    hidden3 = tf.layers.dense(tf.layers.dropout(hidden2, dropout_rate, training=training), n_hidden3, name=\"hidden3\",\n",
    "                              activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(tf.layers.dropout(hidden3, dropout_rate, training=training), n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "logdir = log_dir(\"mnist_dnn\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "m, n = X_train.shape\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/mnist_relu.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./mnist_relu\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Previous train was interrupted, will start at epoch {}\".format(start_epoch))\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation set accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"termination\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0599ee1b8fd2ba1763ecac8b5e0ff5cb192bae3c"
   },
   "source": [
    "## Dropout NN 3 Layers(Logistic) 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "842f8553ec5637543d305e9998213291f4128c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation set accuracy: 11.260% \tLoss: 2.30426\n",
      "Epoch: 5 \tValidation set accuracy: 54.460% \tLoss: 1.51349\n",
      "Epoch: 10 \tValidation set accuracy: 82.520% \tLoss: 0.62264\n",
      "Epoch: 15 \tValidation set accuracy: 86.900% \tLoss: 0.44661\n",
      "Epoch: 20 \tValidation set accuracy: 89.080% \tLoss: 0.38989\n",
      "Epoch: 25 \tValidation set accuracy: 89.820% \tLoss: 0.35892\n",
      "Epoch: 30 \tValidation set accuracy: 90.660% \tLoss: 0.33679\n",
      "Epoch: 35 \tValidation set accuracy: 90.960% \tLoss: 0.32228\n",
      "Epoch: 40 \tValidation set accuracy: 91.260% \tLoss: 0.31592\n",
      "Epoch: 45 \tValidation set accuracy: 91.720% \tLoss: 0.30399\n",
      "Epoch: 50 \tValidation set accuracy: 91.980% \tLoss: 0.29383\n",
      "Epoch: 55 \tValidation set accuracy: 91.940% \tLoss: 0.28481\n",
      "Epoch: 60 \tValidation set accuracy: 92.180% \tLoss: 0.27952\n",
      "Epoch: 65 \tValidation set accuracy: 92.260% \tLoss: 0.27179\n",
      "Epoch: 70 \tValidation set accuracy: 92.360% \tLoss: 0.26453\n",
      "Epoch: 75 \tValidation set accuracy: 92.600% \tLoss: 0.25924\n",
      "Epoch: 80 \tValidation set accuracy: 92.500% \tLoss: 0.25503\n",
      "Epoch: 85 \tValidation set accuracy: 92.380% \tLoss: 0.25192\n",
      "Epoch: 90 \tValidation set accuracy: 93.060% \tLoss: 0.24174\n",
      "Epoch: 95 \tValidation set accuracy: 92.940% \tLoss: 0.24096\n",
      "Epoch: 100 \tValidation set accuracy: 93.100% \tLoss: 0.22918\n",
      "Epoch: 105 \tValidation set accuracy: 93.280% \tLoss: 0.23405\n",
      "Epoch: 110 \tValidation set accuracy: 93.620% \tLoss: 0.22226\n",
      "Epoch: 115 \tValidation set accuracy: 93.620% \tLoss: 0.21566\n",
      "Epoch: 120 \tValidation set accuracy: 93.740% \tLoss: 0.21006\n",
      "Epoch: 125 \tValidation set accuracy: 94.140% \tLoss: 0.20398\n",
      "Epoch: 130 \tValidation set accuracy: 94.400% \tLoss: 0.19869\n",
      "Epoch: 135 \tValidation set accuracy: 94.340% \tLoss: 0.19412\n",
      "Epoch: 140 \tValidation set accuracy: 94.500% \tLoss: 0.19264\n",
      "Epoch: 145 \tValidation set accuracy: 94.800% \tLoss: 0.18139\n",
      "Epoch: 150 \tValidation set accuracy: 94.960% \tLoss: 0.17969\n",
      "Epoch: 155 \tValidation set accuracy: 95.000% \tLoss: 0.17827\n",
      "Epoch: 160 \tValidation set accuracy: 95.340% \tLoss: 0.17050\n",
      "Epoch: 165 \tValidation set accuracy: 95.340% \tLoss: 0.16717\n",
      "Epoch: 170 \tValidation set accuracy: 95.440% \tLoss: 0.16231\n",
      "Epoch: 175 \tValidation set accuracy: 95.740% \tLoss: 0.15544\n",
      "Epoch: 180 \tValidation set accuracy: 95.800% \tLoss: 0.15278\n",
      "Epoch: 185 \tValidation set accuracy: 95.880% \tLoss: 0.15384\n",
      "Epoch: 190 \tValidation set accuracy: 95.880% \tLoss: 0.14918\n",
      "Epoch: 195 \tValidation set accuracy: 96.000% \tLoss: 0.14683\n",
      "Epoch: 200 \tValidation set accuracy: 96.200% \tLoss: 0.13939\n",
      "Epoch: 205 \tValidation set accuracy: 96.120% \tLoss: 0.13832\n",
      "Epoch: 210 \tValidation set accuracy: 96.200% \tLoss: 0.13661\n",
      "Epoch: 215 \tValidation set accuracy: 96.160% \tLoss: 0.13246\n",
      "Epoch: 220 \tValidation set accuracy: 96.300% \tLoss: 0.13099\n",
      "Epoch: 225 \tValidation set accuracy: 96.400% \tLoss: 0.13076\n",
      "Epoch: 230 \tValidation set accuracy: 96.480% \tLoss: 0.12432\n",
      "Epoch: 235 \tValidation set accuracy: 96.460% \tLoss: 0.12455\n",
      "Epoch: 240 \tValidation set accuracy: 96.540% \tLoss: 0.12260\n",
      "Epoch: 245 \tValidation set accuracy: 96.580% \tLoss: 0.12026\n",
      "Epoch: 250 \tValidation set accuracy: 96.440% \tLoss: 0.12058\n",
      "Epoch: 255 \tValidation set accuracy: 96.640% \tLoss: 0.11991\n",
      "Epoch: 260 \tValidation set accuracy: 96.580% \tLoss: 0.11869\n",
      "Epoch: 265 \tValidation set accuracy: 96.600% \tLoss: 0.11979\n",
      "Epoch: 270 \tValidation set accuracy: 96.740% \tLoss: 0.11542\n",
      "Epoch: 275 \tValidation set accuracy: 96.880% \tLoss: 0.11088\n",
      "Epoch: 280 \tValidation set accuracy: 96.860% \tLoss: 0.10880\n",
      "Epoch: 285 \tValidation set accuracy: 96.780% \tLoss: 0.11056\n",
      "Epoch: 290 \tValidation set accuracy: 96.980% \tLoss: 0.10757\n",
      "Epoch: 295 \tValidation set accuracy: 97.000% \tLoss: 0.10713\n",
      "Epoch: 300 \tValidation set accuracy: 96.900% \tLoss: 0.10651\n",
      "Epoch: 305 \tValidation set accuracy: 96.940% \tLoss: 0.10631\n",
      "Epoch: 310 \tValidation set accuracy: 96.840% \tLoss: 0.10480\n",
      "Epoch: 315 \tValidation set accuracy: 96.960% \tLoss: 0.10565\n",
      "Epoch: 320 \tValidation set accuracy: 97.140% \tLoss: 0.10268\n",
      "Epoch: 325 \tValidation set accuracy: 97.020% \tLoss: 0.10125\n",
      "Epoch: 330 \tValidation set accuracy: 96.980% \tLoss: 0.10418\n",
      "Epoch: 335 \tValidation set accuracy: 97.100% \tLoss: 0.10114\n",
      "Epoch: 340 \tValidation set accuracy: 97.080% \tLoss: 0.09927\n",
      "Epoch: 345 \tValidation set accuracy: 97.060% \tLoss: 0.10073\n",
      "Epoch: 350 \tValidation set accuracy: 97.200% \tLoss: 0.09702\n",
      "Epoch: 355 \tValidation set accuracy: 97.060% \tLoss: 0.09676\n",
      "Epoch: 360 \tValidation set accuracy: 97.140% \tLoss: 0.09787\n",
      "Epoch: 365 \tValidation set accuracy: 97.100% \tLoss: 0.09840\n",
      "termination\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 28*28\n",
    "n_hidden1 = 1024\n",
    "n_hidden2 = 1024\n",
    "n_hidden3 = 1024\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# Set the dropout rate\n",
    "dropout_rate = 0.5  \n",
    "X_drop = tf.layers.dropout(X, 0.8, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.sigmoid)\n",
    "    hidden2 = tf.layers.dense(tf.layers.dropout(hidden1, dropout_rate, training=training), n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.sigmoid)\n",
    "    hidden3 = tf.layers.dense(tf.layers.dropout(hidden2, dropout_rate, training=training), n_hidden3, name=\"hidden3\",\n",
    "                              activation=tf.nn.sigmoid)\n",
    "    logits = tf.layers.dense(tf.layers.dropout(hidden3, dropout_rate, training=training), n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "logdir = log_dir(\"mnist_dnn\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "m, n = X_train.shape\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/mnist_logistic.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./mnist_logistic\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Previous train was interrupted, will start at epoch {}\".format(start_epoch))\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation set accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"termination\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout NN 3 Layers(ReLU) with max-norm constraint 1024 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation set accuracy: 91.580% \tLoss: 0.31120\n",
      "Epoch: 5 \tValidation set accuracy: 95.960% \tLoss: 0.14384\n",
      "Epoch: 10 \tValidation set accuracy: 97.520% \tLoss: 0.09419\n",
      "Epoch: 15 \tValidation set accuracy: 97.700% \tLoss: 0.07958\n",
      "Epoch: 20 \tValidation set accuracy: 97.960% \tLoss: 0.06950\n",
      "Epoch: 25 \tValidation set accuracy: 98.040% \tLoss: 0.06496\n",
      "Epoch: 30 \tValidation set accuracy: 98.140% \tLoss: 0.06622\n",
      "Epoch: 35 \tValidation set accuracy: 98.140% \tLoss: 0.06571\n",
      "Epoch: 40 \tValidation set accuracy: 98.240% \tLoss: 0.06448\n",
      "Epoch: 45 \tValidation set accuracy: 98.260% \tLoss: 0.06591\n",
      "Epoch: 50 \tValidation set accuracy: 98.260% \tLoss: 0.06580\n",
      "Epoch: 55 \tValidation set accuracy: 98.380% \tLoss: 0.06711\n",
      "Epoch: 60 \tValidation set accuracy: 98.360% \tLoss: 0.06883\n",
      "Epoch: 65 \tValidation set accuracy: 98.340% \tLoss: 0.06975\n",
      "Epoch: 70 \tValidation set accuracy: 98.380% \tLoss: 0.07062\n",
      "Epoch: 75 \tValidation set accuracy: 98.320% \tLoss: 0.07063\n",
      "Epoch: 80 \tValidation set accuracy: 98.420% \tLoss: 0.07167\n",
      "Epoch: 85 \tValidation set accuracy: 98.460% \tLoss: 0.07212\n",
      "termination\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 1024\n",
    "n_hidden2 = 1024\n",
    "n_hidden3 = 1024\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# Implement dropout\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, 0.8, training=training)\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold = 1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu, kernel_regularizer=max_norm_reg)\n",
    "    hidden2 = tf.layers.dense(tf.layers.dropout(hidden1, dropout_rate, training=training), n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu, kernel_regularizer=max_norm_reg)\n",
    "    hidden3 = tf.layers.dense(tf.layers.dropout(hidden2, dropout_rate, training=training), n_hidden3, name=\"hidden3\",\n",
    "                              activation=tf.nn.relu, kernel_regularizer=max_norm_reg)\n",
    "    logits = tf.layers.dense(tf.layers.dropout(hidden3, dropout_rate, training=training), n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "logdir = log_dir(\"mnist_dnn\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "m, n = X_train.shape\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/mnist_relu_norm.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./mnist_relu_norm\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Previous train was interrupted, will start at epoch {}\".format(start_epoch))\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation set accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"termination\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout NN 3 Layers(ReLU) with max-norm constraint 2048 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation set accuracy: 91.960% \tLoss: 0.30055\n",
      "Epoch: 5 \tValidation set accuracy: 96.240% \tLoss: 0.14103\n",
      "Epoch: 10 \tValidation set accuracy: 97.500% \tLoss: 0.09193\n",
      "Epoch: 15 \tValidation set accuracy: 97.760% \tLoss: 0.07634\n",
      "Epoch: 20 \tValidation set accuracy: 98.120% \tLoss: 0.06699\n",
      "Epoch: 25 \tValidation set accuracy: 98.100% \tLoss: 0.06271\n",
      "Epoch: 30 \tValidation set accuracy: 98.120% \tLoss: 0.06459\n",
      "Epoch: 35 \tValidation set accuracy: 98.320% \tLoss: 0.06318\n",
      "Epoch: 40 \tValidation set accuracy: 98.280% \tLoss: 0.06130\n",
      "Epoch: 45 \tValidation set accuracy: 98.300% \tLoss: 0.06202\n",
      "Epoch: 50 \tValidation set accuracy: 98.420% \tLoss: 0.06210\n",
      "Epoch: 55 \tValidation set accuracy: 98.380% \tLoss: 0.06355\n",
      "Epoch: 60 \tValidation set accuracy: 98.380% \tLoss: 0.06450\n",
      "Epoch: 65 \tValidation set accuracy: 98.300% \tLoss: 0.06554\n",
      "Epoch: 70 \tValidation set accuracy: 98.300% \tLoss: 0.06635\n",
      "Epoch: 75 \tValidation set accuracy: 98.360% \tLoss: 0.06572\n",
      "Epoch: 80 \tValidation set accuracy: 98.340% \tLoss: 0.06655\n",
      "Epoch: 85 \tValidation set accuracy: 98.380% \tLoss: 0.06716\n",
      "termination\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "n_inputs = 28*28  \n",
    "n_hidden1 = 2048\n",
    "n_hidden2 = 2048\n",
    "n_hidden3 = 2048\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# Implement dropout\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, 0.8, training=training)\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold = 1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu, kernel_regularizer=max_norm_reg)\n",
    "    hidden2 = tf.layers.dense(tf.layers.dropout(hidden1, dropout_rate, training=training), n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu, kernel_regularizer=max_norm_reg)\n",
    "    hidden3 = tf.layers.dense(tf.layers.dropout(hidden2, dropout_rate, training=training), n_hidden3, name=\"hidden3\",\n",
    "                              activation=tf.nn.relu, kernel_regularizer=max_norm_reg)\n",
    "    logits = tf.layers.dense(tf.layers.dropout(hidden3, dropout_rate, training=training), n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "logdir = log_dir(\"mnist_dnn\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "m, n = X_train.shape\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/mnist_relu_norm2048.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./mnist_relu_norm2048\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Previous train was interrupted, will start at epoch {}\".format(start_epoch))\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation set accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"termination\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout NN 2 Layers(ReLU) with max-norm constraint 4096 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation set accuracy: 91.440% \tLoss: 0.32339\n",
      "Epoch: 5 \tValidation set accuracy: 95.520% \tLoss: 0.16703\n",
      "Epoch: 10 \tValidation set accuracy: 96.900% \tLoss: 0.11745\n",
      "Epoch: 15 \tValidation set accuracy: 97.500% \tLoss: 0.09419\n",
      "Epoch: 20 \tValidation set accuracy: 97.800% \tLoss: 0.08185\n",
      "Epoch: 25 \tValidation set accuracy: 97.920% \tLoss: 0.07378\n",
      "Epoch: 30 \tValidation set accuracy: 98.100% \tLoss: 0.07026\n",
      "Epoch: 35 \tValidation set accuracy: 98.140% \tLoss: 0.06700\n",
      "Epoch: 40 \tValidation set accuracy: 98.180% \tLoss: 0.06405\n",
      "Epoch: 45 \tValidation set accuracy: 98.280% \tLoss: 0.06229\n",
      "Epoch: 50 \tValidation set accuracy: 98.340% \tLoss: 0.06153\n",
      "Epoch: 55 \tValidation set accuracy: 98.240% \tLoss: 0.06114\n",
      "Epoch: 60 \tValidation set accuracy: 98.280% \tLoss: 0.06186\n",
      "Epoch: 65 \tValidation set accuracy: 98.360% \tLoss: 0.06145\n",
      "Epoch: 70 \tValidation set accuracy: 98.300% \tLoss: 0.06154\n",
      "Epoch: 75 \tValidation set accuracy: 98.380% \tLoss: 0.06072\n",
      "Epoch: 80 \tValidation set accuracy: 98.420% \tLoss: 0.06114\n",
      "Epoch: 85 \tValidation set accuracy: 98.420% \tLoss: 0.06120\n",
      "Epoch: 90 \tValidation set accuracy: 98.440% \tLoss: 0.06150\n",
      "Epoch: 95 \tValidation set accuracy: 98.420% \tLoss: 0.06261\n",
      "Epoch: 100 \tValidation set accuracy: 98.440% \tLoss: 0.06217\n",
      "Epoch: 105 \tValidation set accuracy: 98.440% \tLoss: 0.06313\n",
      "Epoch: 110 \tValidation set accuracy: 98.500% \tLoss: 0.06322\n",
      "Epoch: 115 \tValidation set accuracy: 98.480% \tLoss: 0.06341\n",
      "termination\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "n_inputs = 28*28  \n",
    "n_hidden1 = 4096\n",
    "n_hidden2 = 4096\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# Implement dropout\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, 0.8, training=training)\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold = 1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu, kernel_regularizer=max_norm_reg)\n",
    "    hidden2 = tf.layers.dense(tf.layers.dropout(hidden1, dropout_rate, training=training), n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu, kernel_regularizer=max_norm_reg)\n",
    "    logits = tf.layers.dense(tf.layers.dropout(hidden2, dropout_rate, training=training), n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "logdir = log_dir(\"mnist_dnn\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "m, n = X_train.shape\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/mnist_relu_norm4096.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./mnist_relu_norm4096\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Previous train was interrupted, will start at epoch {}\".format(start_epoch))\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation set accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"termination\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout NN 2 Layers(ReLU) with max-norm constraint 8192 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation set accuracy: 91.640% \tLoss: 0.31973\n",
      "Epoch: 5 \tValidation set accuracy: 95.520% \tLoss: 0.16414\n",
      "Epoch: 10 \tValidation set accuracy: 97.040% \tLoss: 0.11532\n",
      "Epoch: 15 \tValidation set accuracy: 97.540% \tLoss: 0.09256\n",
      "Epoch: 20 \tValidation set accuracy: 97.860% \tLoss: 0.08076\n",
      "Epoch: 25 \tValidation set accuracy: 98.000% \tLoss: 0.07212\n",
      "Epoch: 30 \tValidation set accuracy: 98.040% \tLoss: 0.06872\n",
      "Epoch: 35 \tValidation set accuracy: 98.100% \tLoss: 0.06528\n",
      "Epoch: 40 \tValidation set accuracy: 98.280% \tLoss: 0.06236\n",
      "Epoch: 45 \tValidation set accuracy: 98.280% \tLoss: 0.06020\n",
      "Epoch: 50 \tValidation set accuracy: 98.300% \tLoss: 0.05943\n",
      "Epoch: 55 \tValidation set accuracy: 98.300% \tLoss: 0.05909\n",
      "Epoch: 60 \tValidation set accuracy: 98.360% \tLoss: 0.05978\n",
      "Epoch: 65 \tValidation set accuracy: 98.240% \tLoss: 0.05911\n",
      "Epoch: 70 \tValidation set accuracy: 98.360% \tLoss: 0.05897\n",
      "Epoch: 75 \tValidation set accuracy: 98.420% \tLoss: 0.05832\n",
      "Epoch: 80 \tValidation set accuracy: 98.340% \tLoss: 0.05867\n",
      "Epoch: 85 \tValidation set accuracy: 98.420% \tLoss: 0.05876\n",
      "Epoch: 90 \tValidation set accuracy: 98.380% \tLoss: 0.05897\n",
      "Epoch: 95 \tValidation set accuracy: 98.380% \tLoss: 0.06001\n",
      "Epoch: 100 \tValidation set accuracy: 98.380% \tLoss: 0.05962\n",
      "Epoch: 105 \tValidation set accuracy: 98.480% \tLoss: 0.06054\n",
      "Epoch: 110 \tValidation set accuracy: 98.440% \tLoss: 0.06048\n",
      "Epoch: 115 \tValidation set accuracy: 98.420% \tLoss: 0.06077\n",
      "Epoch: 120 \tValidation set accuracy: 98.500% \tLoss: 0.06147\n",
      "termination\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "n_inputs = 28*28  \n",
    "n_hidden1 = 8192\n",
    "n_hidden2 = 8192\n",
    "n_outputs = 10\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "# Implement dropout\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, 0.8, training=training)\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold = 1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, name=\"hidden1\",\n",
    "                              activation=tf.nn.relu, kernel_regularizer=max_norm_reg)\n",
    "    hidden2 = tf.layers.dense(tf.layers.dropout(hidden1, dropout_rate, training=training), n_hidden2, name=\"hidden2\",\n",
    "                              activation=tf.nn.relu, kernel_regularizer=max_norm_reg)\n",
    "    logits = tf.layers.dense(tf.layers.dropout(hidden2, dropout_rate, training=training), n_outputs, name=\"outputs\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)\n",
    "logdir = log_dir(\"mnist_dnn\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "m, n = X_train.shape\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/mnist_relu_norm8192.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./mnist_relu_norm8192\"\n",
    "\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Previous train was interrupted, will start at epoch {}\".format(start_epoch))\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run([accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_valid, y: y_valid})\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch:\", epoch,\n",
    "                  \"\\tValidation set accuracy: {:.3f}%\".format(accuracy_val * 100),\n",
    "                  \"\\tLoss: {:.5f}\".format(loss_val))\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "            if loss_val < best_loss:\n",
    "                saver.save(sess, final_model_path)\n",
    "                best_loss = loss_val\n",
    "            else:\n",
    "                epochs_without_progress += 5\n",
    "                if epochs_without_progress > max_epochs_without_progress:\n",
    "                    print(\"termination\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
